{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc7be446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import json\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from codecarbon import EmissionsTracker\n",
    "import mlflow\n",
    "from typing import List\n",
    "import ollama\n",
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "from mlflow.entities import Feedback\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from recruitair.job_offers.models import KeyCriteriaResponse\n",
    "\n",
    "os.environ[\"codecarbon_log_level\"] = \"WARNING\"  # Disable most of the loggings\n",
    "os.environ[\"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\"] = \"true\"\n",
    "mlflow.set_tracking_uri(\"https://ml-4cb370e118ec407c83eed254868ebce1.ecs.eu-north-1.on.aws/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa77edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_PROMPT_NAME = \"one-shot-long-descriptions\"\n",
    "MLFLOW_PROMPT_VERSION = 1\n",
    "OLLAMA_MODEL = \"dolphin3\"\n",
    "OLLAMA_MODEL_VERSION = \"8b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fbbe5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/interim/preprocessed_jobs.jsonl\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "job_offers = []\n",
    "criteria = []\n",
    "for line in lines:\n",
    "    job_offer = json.loads(line)\n",
    "    job_offers.append({\"job_offer_text\": job_offer[\"job_description\"]})\n",
    "    criteria.append({\"key_criteria\": job_offer[\"criteria\"]})\n",
    "\n",
    "data = pd.DataFrame({\"inputs\": job_offers, \"expectations\": criteria})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f08b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(job_offer_text: str) -> KeyCriteriaResponse:\n",
    "    llm = ChatOllama(model=f\"{OLLAMA_MODEL}:{OLLAMA_MODEL_VERSION}\", temperature=0)\n",
    "    prompt = mlflow.genai.load_prompt(f\"prompts:/{MLFLOW_PROMPT_NAME}/{MLFLOW_PROMPT_VERSION}\")\n",
    "    response = llm.with_structured_output(prompt.response_format, method=\"json_schema\").invoke(\n",
    "        prompt.format(job_offer_text=job_offer_text)\n",
    "    )\n",
    "    return KeyCriteriaResponse.model_validate(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d71d64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    return np.array(ollama.embed(\"mxbai-embed-large:335m\", input=text)[\"embeddings\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3903983",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mlflow.genai.scorer(name=\"target-recall/embedding/mxbai-embed-large:335m\")\n",
    "def target_recall(outputs: KeyCriteriaResponse, expectations: KeyCriteriaResponse) -> Feedback:\n",
    "    expectations = KeyCriteriaResponse.model_validate(expectations)\n",
    "    # Compute the embeddings of the names of the all the extracted and target criteria:\n",
    "    target_embeddings = []\n",
    "    for target_criterion in expectations.key_criteria:\n",
    "        target_embeddings.append(get_embedding(target_criterion.name))\n",
    "    response_embeddings = []\n",
    "    for response_criterion in outputs.key_criteria:\n",
    "        response_embeddings.append(get_embedding(response_criterion.name))\n",
    "    # Compute the cosine similarity matrix between the two sets of embeddings:\n",
    "    similarity_matrix = np.inner(np.array(response_embeddings), np.array(target_embeddings))\n",
    "\n",
    "    # We'll score as follows: For each target criterion, we'll find the most similar\n",
    "    # response criterion, thus we'll have, for each target criterion, a score\n",
    "    # between 0 and 1 representing how well it was matched. We'll then floor\n",
    "    # everything below 0.8 to 0, and average the rest.\n",
    "    # This means that if a target criterion was not matched with at least 0.8,\n",
    "    # it will contribute 0 to the average. We'll call this \"target recall score\".\n",
    "    # It can be interpreted as the \"rich fraction\" of target criteria that were well matched.\n",
    "    scores = similarity_matrix.max(axis=0)\n",
    "    target_recall_score = float(np.where(scores < 0.8, 0, scores).mean())\n",
    "    return Feedback(value=target_recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c08d9f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mlflow.genai.scorer(name=\"importance-mse/embedding/mxbai-embed-large:335m\")\n",
    "def importance_mse(outputs: KeyCriteriaResponse, expectations: KeyCriteriaResponse) -> Feedback:\n",
    "    expectations = KeyCriteriaResponse.model_validate(expectations)\n",
    "    # Compute the embeddings of the names of the all the extracted and target criteria:\n",
    "    target_embeddings = []\n",
    "    for target_criterion in expectations.key_criteria:\n",
    "        target_embeddings.append(get_embedding(target_criterion.name))\n",
    "    response_embeddings = []\n",
    "    for response_criterion in outputs.key_criteria:\n",
    "        response_embeddings.append(get_embedding(response_criterion.name))\n",
    "    # Compute the cosine similarity matrix between the two sets of embeddings:\n",
    "    similarity_matrix = np.inner(np.array(response_embeddings), np.array(target_embeddings))\n",
    "\n",
    "    # Find which target is more likely to be for each response:\n",
    "    response_to_target: List[int] = similarity_matrix.argmax(axis=1)\n",
    "    response_to_target[similarity_matrix.max(axis=1) < 0.5] = -1  # Consider as unmatched if similarity < 0.5\n",
    "    # Compute the mean square error of matched importance:\n",
    "    mse = 0\n",
    "    for i in range(len(outputs.key_criteria)):\n",
    "        if response_to_target[i] == -1:\n",
    "            mse += 100**2  # Penalize completely unmatched criteria\n",
    "            continue\n",
    "        response_criterion = outputs.key_criteria[i]\n",
    "        target_criterion = expectations.key_criteria[response_to_target[i]]\n",
    "        mse += (response_criterion.importance - target_criterion.importance) ** 2\n",
    "    mse /= len(outputs.key_criteria)\n",
    "\n",
    "    return Feedback(value=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c7323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/03 20:32:12 INFO mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics. Set logger level to DEBUG for more details.\n",
      "2025/12/03 20:32:12 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n",
      "[codecarbon WARNING @ 20:32:13] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon WARNING @ 20:32:15] We saw that you have a Intel(R) Core(TM) Ultra 7 255H but we don't know it. Please contact us.\n",
      "[codecarbon WARNING @ 20:32:15] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon WARNING @ 20:32:15] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "2025/12/03 20:32:15 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n",
      "2025/12/03 20:32:15 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n",
      "2025/12/03 20:32:16 WARNING mlflow.tracing.fluent: Failed to start span RunnableSequence: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n",
      "c:\\Users\\UPCnet\\Documents\\MDS\\MLOps\\CriteriaExtractor\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\UPCnet\\Documents\\MDS\\MLOps\\CriteriaExtractor\\.venv\\Lib\\site-packages\\mlflow\\prompt\\registry_utils.py:144: FutureWarning: The `mlflow.load_prompt` API is moved to the `mlflow.genai` namespace. Please use `mlflow.genai.load_prompt` instead. The original API will be removed in the future release.\n",
      "  return func(*args, **kwargs)\n",
      "Evaluating:   0%|          | 0/849 [Elapsed: 00:00, Remaining: ?] "
     ]
    }
   ],
   "source": [
    "mlflow.langchain.autolog()\n",
    "mlflow.set_experiment(f\"criteria-extraction/{MLFLOW_PROMPT_NAME}\")\n",
    "\n",
    "with mlflow.start_run(\n",
    "    run_name=f\"eval-{MLFLOW_PROMPT_NAME}-v{MLFLOW_PROMPT_VERSION}-{OLLAMA_MODEL}-v{OLLAMA_MODEL_VERSION}\"\n",
    ") as run:\n",
    "\n",
    "    mlflow.log_param(\"emissions-tracker/measure-power-secs\", 1)\n",
    "    mlflow.log_param(\"emissions-tracker/tracking-mode\", \"process\")\n",
    "    mlflow.log_param(\"ollama_model\", OLLAMA_MODEL)\n",
    "    mlflow.log_param(\"ollama_model_version\", OLLAMA_MODEL_VERSION)\n",
    "    mlflow.log_param(\"temperature\", 0)\n",
    "    mlflow.log_param(\"mlflow_prompt_name\", MLFLOW_PROMPT_NAME)\n",
    "    mlflow.log_param(\"mlflow_prompt_version\", MLFLOW_PROMPT_VERSION)\n",
    "\n",
    "    tracker = EmissionsTracker(measure_power_secs=1, tracking_mode=\"process\", save_to_file=False)\n",
    "    tracker.start()\n",
    "    mlflow.genai.evaluate(\n",
    "        predict_fn=predict,\n",
    "        data=data,\n",
    "        scorers=[\n",
    "            target_recall,\n",
    "            importance_mse,\n",
    "        ],\n",
    "    )\n",
    "    tracker.stop()\n",
    "    all_metrics = tracker.final_emissions_data.values\n",
    "    num_metrics = {f\"emissions-tracker/{k}\": v for k, v in all_metrics.items() if isinstance(v, (int, float))}\n",
    "    mlflow.log_metrics(num_metrics, run_id=run.info.run_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-recruitair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
