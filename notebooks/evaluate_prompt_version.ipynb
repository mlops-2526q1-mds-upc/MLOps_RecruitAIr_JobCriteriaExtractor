{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc7be446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import json\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from codecarbon import EmissionsTracker\n",
    "import mlflow\n",
    "from typing import List\n",
    "import ollama\n",
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "from mlflow.entities import Feedback\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from recruitair.job_offers.models import KeyCriteriaResponse\n",
    "\n",
    "os.environ[\"codecarbon_log_level\"] = \"WARNING\"  # Disable most of the loggings\n",
    "os.environ[\"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\"] = \"true\"\n",
    "mlflow.set_tracking_uri(\"http://nattech.fib.upc.edu:40380/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa77edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_PROMPT_NAME = \"criteria-extraction\"\n",
    "MLFLOW_PROMPT_VERSION = 1\n",
    "OLLAMA_MODEL = \"dolphin3\"\n",
    "OLLAMA_MODEL_VERSION = \"8b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fbbe5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/interim/preprocessed_jobs.jsonl\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "job_offers = []\n",
    "criteria = []\n",
    "for line in lines:\n",
    "    job_offer = json.loads(line)\n",
    "    job_offers.append({\"job_offer_text\": job_offer[\"job_description\"]})\n",
    "    criteria.append({\"key_criteria\": job_offer[\"criteria\"]})\n",
    "\n",
    "data = pd.DataFrame({\"inputs\": job_offers, \"expectations\": criteria})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f08b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(job_offer_text: str) -> KeyCriteriaResponse:\n",
    "    llm = ChatOllama(model=f\"{OLLAMA_MODEL}:{OLLAMA_MODEL_VERSION}\", temperature=0)\n",
    "    prompt = mlflow.genai.load_prompt(f\"prompts:/{MLFLOW_PROMPT_NAME}/{MLFLOW_PROMPT_VERSION}\")\n",
    "    response = llm.with_structured_output(prompt.response_format, method=\"json_schema\").invoke(\n",
    "        prompt.format(job_offer_text=job_offer_text)\n",
    "    )\n",
    "    return KeyCriteriaResponse.model_validate(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d71d64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    return np.array(ollama.embed(\"mxbai-embed-large:335m\", input=text)[\"embeddings\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3903983",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mlflow.genai.scorer(name=\"target-recall/embedding/mxbai-embed-large:335m\")\n",
    "def target_recall(outputs: KeyCriteriaResponse, expectations: KeyCriteriaResponse) -> Feedback:\n",
    "    expectations = KeyCriteriaResponse.model_validate(expectations)\n",
    "    # Compute the embeddings of the names of the all the extracted and target criteria:\n",
    "    target_embeddings = []\n",
    "    for target_criterion in expectations.key_criteria:\n",
    "        target_embeddings.append(get_embedding(target_criterion.name))\n",
    "    response_embeddings = []\n",
    "    for response_criterion in outputs.key_criteria:\n",
    "        response_embeddings.append(get_embedding(response_criterion.name))\n",
    "    # Compute the cosine similarity matrix between the two sets of embeddings:\n",
    "    similarity_matrix = np.inner(np.array(response_embeddings), np.array(target_embeddings))\n",
    "\n",
    "    # We'll score as follows: For each target criterion, we'll find the most similar\n",
    "    # response criterion, thus we'll have, for each target criterion, a score\n",
    "    # between 0 and 1 representing how well it was matched. We'll then floor\n",
    "    # everything below 0.8 to 0, and average the rest.\n",
    "    # This means that if a target criterion was not matched with at least 0.8,\n",
    "    # it will contribute 0 to the average. We'll call this \"target recall score\".\n",
    "    # It can be interpreted as the \"rich fraction\" of target criteria that were well matched.\n",
    "    scores = similarity_matrix.max(axis=0)\n",
    "    target_recall_score = float(np.where(scores < 0.8, 0, scores).mean())\n",
    "    return Feedback(value=target_recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c08d9f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mlflow.genai.scorer(name=\"importance-mse/embedding/mxbai-embed-large:335m\")\n",
    "def importance_mse(outputs: KeyCriteriaResponse, expectations: KeyCriteriaResponse) -> Feedback:\n",
    "    expectations = KeyCriteriaResponse.model_validate(expectations)\n",
    "    # Compute the embeddings of the names of the all the extracted and target criteria:\n",
    "    target_embeddings = []\n",
    "    for target_criterion in expectations.key_criteria:\n",
    "        target_embeddings.append(get_embedding(target_criterion.name))\n",
    "    response_embeddings = []\n",
    "    for response_criterion in outputs.key_criteria:\n",
    "        response_embeddings.append(get_embedding(response_criterion.name))\n",
    "    # Compute the cosine similarity matrix between the two sets of embeddings:\n",
    "    similarity_matrix = np.inner(np.array(response_embeddings), np.array(target_embeddings))\n",
    "\n",
    "    # Find which target is more likely to be for each response:\n",
    "    response_to_target: List[int] = similarity_matrix.argmax(axis=1)\n",
    "    response_to_target[similarity_matrix.max(axis=1) < 0.5] = -1  # Consider as unmatched if similarity < 0.5\n",
    "    # Compute the mean square error of matched importance:\n",
    "    mse = 0\n",
    "    for i in range(len(outputs.key_criteria)):\n",
    "        if response_to_target[i] == -1:\n",
    "            mse += 100**2  # Penalize completely unmatched criteria\n",
    "            continue\n",
    "        response_criterion = outputs.key_criteria[i]\n",
    "        target_criterion = expectations.key_criteria[response_to_target[i]]\n",
    "        mse += (response_criterion.importance - target_criterion.importance) ** 2\n",
    "    mse /= len(outputs.key_criteria)\n",
    "\n",
    "    return Feedback(value=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d81c7323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/06 22:48:39 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n",
      "[codecarbon WARNING @ 22:48:40] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon WARNING @ 22:48:42] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon WARNING @ 22:48:42] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "2025/10/06 22:48:42 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n",
      "2025/10/06 22:48:42 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset.\n",
      "c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\MLOps\\CriteriaExtractor\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Evaluating:  41%|████      | 347/849 [Elapsed: 16:01, Remaining: 23:11] [codecarbon WARNING @ 23:04:55] Failed to retrieve gpu total energy consumption\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\MLOps\\CriteriaExtractor\\.venv\\Lib\\site-packages\\codecarbon\\core\\gpu.py\", line 116, in _get_total_energy_consumption\n",
      "    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\MLOps\\CriteriaExtractor\\.venv\\Lib\\site-packages\\pynvml.py\", line 3601, in nvmlDeviceGetTotalEnergyConsumption\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"c:\\Users\\mirxm\\Storage\\Work\\MDS\\S3\\MLOps\\CriteriaExtractor\\.venv\\Lib\\site-packages\\pynvml.py\", line 1061, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "[codecarbon WARNING @ 23:23:33] Background scheduler didn't run for a long period (1118s), results might be inaccurate\n",
      "[codecarbon WARNING @ 23:23:34] Background scheduler didn't run for a long period (1119s), results might be inaccurate\n",
      "[codecarbon WARNING @ 23:23:35] Background scheduler didn't run for a long period (1120s), results might be inaccurate\n",
      "Evaluating: 100%|██████████| 849/849 [Elapsed: 1:01:03, Remaining: 00:00] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"http://nattech.fib.upc.edu:40380/#/experiments/1/runs/2413b06bab71450280da3ce5a7b9cc5b/traces\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/06 23:50:04 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2025/10/06 23:50:04 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    }
   ],
   "source": [
    "mlflow.langchain.autolog()\n",
    "mlflow.set_experiment(\"criteria-extraction\")\n",
    "\n",
    "with mlflow.start_run(\n",
    "    run_name=f\"eval-{MLFLOW_PROMPT_NAME}-v{MLFLOW_PROMPT_VERSION}-{OLLAMA_MODEL}-v{OLLAMA_MODEL_VERSION}\"\n",
    ") as run:\n",
    "\n",
    "    mlflow.log_param(\"emissions-tracker/measure-power-secs\", 1)\n",
    "    mlflow.log_param(\"emissions-tracker/tracking-mode\", \"process\")\n",
    "    mlflow.log_param(\"ollama_model\", OLLAMA_MODEL)\n",
    "    mlflow.log_param(\"ollama_model_version\", OLLAMA_MODEL_VERSION)\n",
    "    mlflow.log_param(\"temperature\", 0)\n",
    "    mlflow.log_param(\"mlflow_prompt_name\", MLFLOW_PROMPT_NAME)\n",
    "    mlflow.log_param(\"mlflow_prompt_version\", MLFLOW_PROMPT_VERSION)\n",
    "\n",
    "    tracker = EmissionsTracker(measure_power_secs=1, tracking_mode=\"process\", save_to_file=False)\n",
    "    tracker.start()\n",
    "    mlflow.genai.evaluate(\n",
    "        predict_fn=predict,\n",
    "        data=data,\n",
    "        scorers=[\n",
    "            target_recall,\n",
    "            importance_mse,\n",
    "        ],\n",
    "    )\n",
    "    tracker.stop()\n",
    "    all_metrics = tracker.final_emissions_data.values\n",
    "    num_metrics = {f\"emissions-tracker/{k}\": v for k, v in all_metrics.items() if isinstance(v, (int, float))}\n",
    "    mlflow.log_metrics(num_metrics, run_id=run.info.run_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CriteriaExtractor (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
